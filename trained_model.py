# -*- coding: utf-8 -*-
"""trained_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MFhuMAcZIhHPEeLw5ExwmSaQQXY_c7V2
"""

# Import necessary libraries
import numpy as np
import pandas as pd
import yfinance as yf
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score

# Fetch market data
def fetch_market_data():
    tickers = ["^GSPC", "BND", "GLD", "BTC-USD"]  # S&P 500, Bonds, Gold, Bitcoin
    data = yf.download(tickers, start="2020-01-01", end="2024-03-01")["Close"]
    returns = data.pct_change().dropna()  # Calculate daily returns
    trends = returns.rolling(window=7).mean().dropna()  # 7-day rolling average trends
    return trends

# Generate synthetic user profiles with real market trends
def generate_user_profiles(n=1000):
    np.random.seed(42)

    # Fetch market data
    market_data = fetch_market_data()

    # Generate synthetic user data
    age = np.random.randint(18, 65, n)
    income = np.random.randint(20000, 150000, n)
    risk_tolerance = np.random.choice(["Low", "Medium", "High"], n, p=[0.3, 0.5, 0.2])

    # Sample market trends from the fetched data
    sample_indices = np.random.choice(len(market_data), n)
    stock_trend = market_data.iloc[sample_indices, 0].values  # S&P 500 trend
    bond_trend = market_data.iloc[sample_indices, 1].values  # Bonds trend
    gold_trend = market_data.iloc[sample_indices, 2].values  # Gold trend
    crypto_trend = market_data.iloc[sample_indices, 3].values  # Bitcoin trend

    # Portfolio allocation based on risk tolerance with added noise
    portfolio_allocation = []

    for risk in risk_tolerance:
        if risk == "Low":
            base_allocation = [10, 70, 15, 5]  # More bonds, less crypto
        elif risk == "Medium":
            base_allocation = [40, 40, 15, 5]  # Balanced
        else:  # High risk
            base_allocation = [70, 10, 10, 10]  # More stocks & crypto

        # Add noise to the allocation
        noise = np.random.normal(0, 5, 4)  # Adding noise with mean 0 and std dev 5
        allocation = np.clip(base_allocation + noise, 0, 100)  # Ensure allocations are between 0% and 100%
        allocation = allocation / np.sum(allocation) * 100  # Normalize to sum to 100%
        portfolio_allocation.append(allocation)

    portfolio_allocation = np.array(portfolio_allocation)  # Convert to NumPy array

    # Create DataFrame
    df = pd.DataFrame({
        "Age": age,
        "Income": income,
        "Risk_Tolerance": risk_tolerance,
        "Stock_Trend": stock_trend,
        "Bond_Trend": bond_trend,
        "Gold_Trend": gold_trend,
        "Crypto_Trend": crypto_trend,
        "Stocks_%": portfolio_allocation[:, 0],
        "Bonds_%": portfolio_allocation[:, 1],
        "Gold_%": portfolio_allocation[:, 2],
        "Crypto_%": portfolio_allocation[:, 3],
    })

    # Encode categorical features
    df["Risk_Tolerance"] = df["Risk_Tolerance"].map({"Low": 0, "Medium": 1, "High": 2})

    return df

# Train and evaluate the Random Forest model
def train_random_forest_model():
    # Generate synthetic data
    user_data = generate_user_profiles(n=5000)  # Increase dataset size

    # Features and target
    X = user_data[["Age", "Income", "Risk_Tolerance", "Stock_Trend", "Bond_Trend", "Gold_Trend", "Crypto_Trend"]]
    Y = user_data[["Stocks_%", "Bonds_%", "Gold_%", "Crypto_%"]]

    # Train-test split
    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

    # Initialize Random Forest model
    rf_model = RandomForestRegressor(random_state=42)

    # Hyperparameter tuning
    param_grid = {
        'n_estimators': [100, 200, 300],
        'max_depth': [None, 10, 20],
        'min_samples_split': [2, 5, 10]
    }
    grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, scoring='r2')
    grid_search.fit(X_train, Y_train)

    # Best model
    best_model = grid_search.best_estimator_
    print("Best Parameters:", grid_search.best_params_)

    # Save the trained model
    joblib.dump(best_model, "random_forest_model.pkl")
    print("Model trained and saved.")

    # Evaluate the model
    Y_pred = best_model.predict(X_test)
    mse = mean_squared_error(Y_test, Y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(Y_test, Y_pred)
    print(f"Model Evaluation:")
    print(f"Mean Squared Error: {mse}")
    print(f"Root Mean Squared Error: {rmse}")
    print(f"R^2 Score: {r2}")

    return best_model